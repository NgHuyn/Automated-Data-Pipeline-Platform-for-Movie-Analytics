{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping 20k Movies in IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.25.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.1.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from selenium) (2023.11.17)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from selenium) (4.9.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Requirement already satisfied: outcome in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\MODERN\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "from bs4 import BeautifulSoup\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class for handling common WebDriver functionalities\n",
    "class BaseScraper:\n",
    "    def __init__(self):\n",
    "        self.driver = self.init_driver()\n",
    "\n",
    "    def init_driver(self):\n",
    "        service = Service()\n",
    "        options = webdriver.ChromeOptions()\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        return driver\n",
    "\n",
    "    def close_driver(self):\n",
    "        self.driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Movie Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoviesScraper class to fetch movies\n",
    "class MoviesScraper(BaseScraper):\n",
    "    def __init__(self, clicks=2):\n",
    "        super().__init__()  # Call the base class constructor\n",
    "        self.clicks = clicks  # Number of times to click the \"50 more\" button\n",
    "        self.movie_data = []\n",
    "\n",
    "    def fetch_movies(self):\n",
    "        url = 'https://www.imdb.com/search/title/?title_type=feature'\n",
    "        self.driver.get(url)\n",
    "\n",
    "        # Click on \"50 more\" button for the specified number of clicks\n",
    "        with tqdm(total=self.clicks, desc='Loading movies') as pbar:\n",
    "            for _ in range(self.clicks):\n",
    "                soup = self.click_see_more_button()  # Re-fetch HTML after each click if successful\n",
    "                # movies = soup.select('div.sc-59c7dc1-2')  # Adjust the selector based on your data structure\n",
    "                # print(f\"Movies loaded after click {_+1}: {len(movies)}\")  # Check the number of loaded movies after each click\n",
    "                pbar.update(1)  # Update the progress bar for each click\n",
    "\n",
    "        # Calculate and apply the wait time based on the number of clicks\n",
    "        wait_time = self._calculate_wait_time(self.clicks)\n",
    "        time.sleep(wait_time)  # Adjust wait time based on clicks\n",
    "\n",
    "        # After all clicks, extract the final set of movie data\n",
    "        html = self.driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        movies = soup.select('div.sc-59c7dc1-2')\n",
    "        self.extract_movie_data(movies)  # Extract all data after final click\n",
    "\n",
    "        self.close_driver()  # Close the driver after fetching movies\n",
    "\n",
    "        return self.movie_data\n",
    "\n",
    "    def click_see_more_button(self):\n",
    "        try:\n",
    "            # Get the current number of 'ipc-title' elements before clicking\n",
    "            initial_elements = self.driver.find_elements(By.CLASS_NAME, 'ipc-title')\n",
    "            initial_count = len(initial_elements)\n",
    "\n",
    "            see_more_button = WebDriverWait(self.driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//span[contains(text(), '50 more')]\"))\n",
    "            )\n",
    "            self.driver.execute_script(\"arguments[0].scrollIntoView(); arguments[0].click();\", see_more_button)\n",
    "\n",
    "            # Retry up to 5 times to confirm that new content has been loaded\n",
    "            for _ in range(5):\n",
    "                current_elements = self.driver.find_elements(By.CLASS_NAME, 'ipc-title')\n",
    "                current_count = len(current_elements)\n",
    "\n",
    "                if current_count > initial_count:\n",
    "                    break  # New content detected\n",
    "                time.sleep(1)  # Brief wait before rechecking\n",
    "\n",
    "            return BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def extract_movie_data(self, movies):\n",
    "        for movie in movies:\n",
    "            title_tag = movie.select_one('h3.ipc-title__text') if movie.select_one('h3.ipc-title__text') else None\n",
    "            link_tag = movie.select_one('a.ipc-title-link-wrapper') if movie.select_one('a.ipc-title-link-wrapper') else None  # if don't have link -> can't find the review or movie id -> skip that movie\n",
    "\n",
    "            title = title_tag.text.strip() if title_tag else 'N/A'\n",
    "            link = link_tag['href']\n",
    "\n",
    "            if link_tag:\n",
    "                link = link_tag['href']\n",
    "\n",
    "                # Extract movie ID\n",
    "                movie_id = link.split('/title/')[1].split('/')[0]\n",
    "            else:\n",
    "                movie_id = 'N/A'\n",
    "\n",
    "            # Clean the title\n",
    "            title = re.sub(r'^\\d+\\.\\s*', '', title)\n",
    "\n",
    "            # Adding the data\n",
    "            self.movie_data.append({\n",
    "                'Movie ID': movie_id,\n",
    "                'Title': title,\n",
    "            })\n",
    "    \n",
    "    def _calculate_wait_time(self, clicks):\n",
    "        \"\"\"\n",
    "        Calculate an adaptive wait time based on the number of clicks.\n",
    "        As the number of clicks increases, the wait time grows exponentially to accommodate website lag.\n",
    "        \"\"\"\n",
    "        base_wait_time = 5  # Base wait time in seconds\n",
    "        growth_factor = 1.2  # Exponential growth factor\n",
    "        additional_wait_time = base_wait_time * (growth_factor ** (clicks // 10))  # Increase wait time every 10 clicks\n",
    "        \n",
    "        return base_wait_time + additional_wait_time\n",
    "    \n",
    "    def save_to_json(self, file_name='movies_data.json'):\n",
    "        with open(file_name, 'w', encoding='utf-8') as file:\n",
    "            json.dump(self.movie_data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading movies: 100%|██████████| 1/1 [00:02<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies fetched and saved to movies.json\n"
     ]
    }
   ],
   "source": [
    "scraper = MoviesScraper(clicks=1)\n",
    "\n",
    "scraper.fetch_movies()\n",
    "scraper.save_to_json()\n",
    "print(\"Movies fetched and saved to movies.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def check_size(file_name):\n",
    "    # Đọc file JSON\n",
    "    with open(os.path.join(os.getcwd(), file_name), 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Kiểm tra kiểu dữ liệu và kích thước\n",
    "    if isinstance(data, list):\n",
    "        print(f\"Số lượng phần tử trong danh sách: {len(data)}\")\n",
    "        # Nếu mỗi phần tử là từ điển, bạn có thể kiểm tra kích thước của từ điển đầu tiên\n",
    "        if data:\n",
    "            print(f\"Kích thước của phần tử đầu tiên: {len(data[0])} thuộc tính\")\n",
    "    elif isinstance(data, dict):\n",
    "        print(f\"Số lượng thuộc tính trong từ điển: {len(data)}\")\n",
    "    else:\n",
    "        print(\"Dữ liệu không phải là danh sách hoặc từ điển.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng phần tử trong danh sách: 100\n",
      "Kích thước của phần tử đầu tiên: 2 thuộc tính\n"
     ]
    }
   ],
   "source": [
    "check_size(file_name='movies_data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Reviews of each Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReviewsScraper class to fetch reviews for each movie\n",
    "class MovieReviewScraper(BaseScraper):\n",
    "    def __init__(self, json_file_path):\n",
    "        super().__init__()  # Call the base class constructor\n",
    "        self.json_file_path = json_file_path  # Path to the JSON file\n",
    "        self.movie_data = self._load_movies_from_json()  # Load movie data from JSON\n",
    "        self.movie_reviews = []  # Adjusted to be a list of movie objects\n",
    "        self.clicks = 0  # Initialize click counter\n",
    "\n",
    "    def _load_movies_from_json(self):\n",
    "        with open(self.json_file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)  # Load JSON data\n",
    "\n",
    "    def fetch_reviews(self):\n",
    "        for movie in self.movie_data:  # Iterate through the list of movies\n",
    "            movie_id = movie['Movie ID']\n",
    "            title = movie['Title']\n",
    "            review_url = f\"https://www.imdb.com/title/{movie_id}/reviews\"\n",
    "            self.driver.get(review_url)\n",
    "\n",
    "            self._load_reviews()\n",
    "\n",
    "            wait_time = self._calculate_wait_time(10, self.clicks)  # Adjust wait time based on click count\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "            html = self.driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            self._extract_reviews(soup, movie_id, title)\n",
    "\n",
    "        self.close_driver()\n",
    "        self.movie_reviews\n",
    "\n",
    "    def _load_reviews(self):\n",
    "        # Try to find and click the 'All' reviews button\n",
    "        try:\n",
    "            all_reviews_button = WebDriverWait(self.driver, 5).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//*[@id='__next']/main/div/section/div/section/div/div[1]/section[1]/div[3]/div/span[2]/button\"))\n",
    "            )\n",
    "            self.driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", all_reviews_button)\n",
    "            time.sleep(2)  # Wait for the page to load if needed\n",
    "            all_reviews_button.click()\n",
    "        except Exception as e:\n",
    "            print(\"Could not find 'All' button, will try to find 'Load More' button.\")\n",
    "            self._load_more_reviews()\n",
    "\n",
    "    def _load_more_reviews(self):\n",
    "        # Add progress bar for loading more reviews\n",
    "        with tqdm(total=10, desc='Loading More Reviews', leave=False) as pbar:\n",
    "            while True:\n",
    "                try:\n",
    "                    load_more_button = WebDriverWait(self.driver, 5).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '//*[@id=\"load-more-trigger\"]'))\n",
    "                    )\n",
    "                    self.driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", load_more_button)\n",
    "                    load_more_button.click()\n",
    "\n",
    "                    self.clicks += 1  # Increment the click count\n",
    "                    pbar.update(1)  # Update progress bar\n",
    "\n",
    "                    wait_time = self._calculate_wait_time(1, self.clicks)  # Adjust wait time based on click count\n",
    "                    time.sleep(wait_time)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"No more 'Load More' buttons to click.\")\n",
    "                    break\n",
    "\n",
    "    def _calculate_wait_time(self, base_wait_time, clicks):\n",
    "        \"\"\"\n",
    "        Calculate an adaptive wait time based on the number of clicks.\n",
    "        As the number of clicks increases, the wait time grows exponentially to accommodate website lag.\n",
    "        \"\"\"\n",
    "        growth_factor = 1.2  # Exponential growth factor\n",
    "        additional_wait_time = base_wait_time * (growth_factor ** (clicks // 10))  # Increase wait time every 10 clicks\n",
    "        \n",
    "        return base_wait_time + additional_wait_time\n",
    "\n",
    "\n",
    "    def _extract_reviews(self, soup, movie_id, title):\n",
    "        reviews = soup.select('article.user-review-item')  # Attempt to extract reviews using one selector\n",
    "        movie_info = {\n",
    "            'Movie ID': movie_id,\n",
    "            'Reviews': []\n",
    "        }\n",
    "        # If no reviews found, try to load more reviews\n",
    "        if not reviews:  \n",
    "            reviews = soup.select('div.lister-item.mode-detail.imdb-user-review')\n",
    "            if not reviews: # If still no reviews available\n",
    "                print(f\"No reviews found for {title}.\")\n",
    "                return\n",
    "\n",
    "            for review in reviews:\n",
    "                parsed_review = self._parse_review(review, \"load_more\")\n",
    "                movie_info['Reviews'].append(parsed_review)\n",
    "        else: # If \"all\" button found\n",
    "            for review in reviews:\n",
    "                parsed_review = self._parse_review(review, \"all\")\n",
    "                movie_info['Reviews'].append(parsed_review)\n",
    "\n",
    "        self.movie_reviews.append(movie_info)\n",
    "\n",
    "        # Count the number of reviews and display it\n",
    "        num_reviews = len(movie_info['Reviews'])\n",
    "        print(f\"Total number of reviews for '{title}': {num_reviews}\")\n",
    "\n",
    "    def _parse_review(self, review, button_type):\n",
    "        \"\"\"\n",
    "        Extract information from the review and return as a dictionary.\n",
    "        \"\"\"\n",
    "        # Extract information from the review based on its type (load_more or all)\n",
    "        if button_type == \"load_more\":\n",
    "            review_rating = review.select_one('span.rating-other-user-rating span').get_text(strip=True) if review.select_one('span.rating-other-user-rating span') else 'No rating'\n",
    "            review_summary = review.select_one('a.title').get_text(strip=True) if review.select_one('a.title') else 'No summary'\n",
    "            review_text = review.select_one('div.text.show-more__control').get_text(strip=True) if review.select_one('div.text.show-more__control') else 'No content'\n",
    "            author_tag = review.select_one('span.display-name-link a').get_text(strip=True) if review.select_one('span.display-name-link a') else 'Unknown Author'\n",
    "            review_date = review.select_one('span.review-date').get_text(strip=True) if review.select_one('span.review-date') else 'No date'\n",
    "        else:\n",
    "            review_rating = review.select_one('span.ipc-rating-star--rating').get_text(strip=True) if review.select_one('span.ipc-rating-star--rating') else 'No rating'\n",
    "            review_summary = review.select_one('span[data-testid=\"review-summary\"]').get_text(strip=True) if review.select_one('span[data-testid=\"review-summary\"]') else 'No summary'\n",
    "            review_text = review.select_one('div.ipc-html-content-inner-div').get_text(strip=True) if review.select_one('div.ipc-html-content-inner-div') else 'No content'\n",
    "            author_tag = review.select_one('a[data-testid=\"author-link\"]').get_text(strip=True) if review.select_one('a[data-testid=\"author-link\"]') else 'Unknown Author'\n",
    "            review_date = review.select_one('li.review-date').get_text(strip=True) if review.select_one('li.review-date') else 'No date'\n",
    "\n",
    "        # Return the review information in the expected format\n",
    "        return {\n",
    "            'Review Summary': review_summary,\n",
    "            'Review': review_text,\n",
    "            'Rating': review_rating,\n",
    "            'Author': author_tag,\n",
    "            'Date': review_date\n",
    "        }\n",
    "    def save_to_json(self):\n",
    "        with open('movies_reviews.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.movie_reviews, f, ensure_ascii=False, indent=4)\n",
    "        print(\"Reviews saved to movies_reviews.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1 movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find 'All' button, will try to find 'Load More' button.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more 'Load More' buttons to click.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews for 'The Substance': 433\n",
      "Reviews saved to movies_reviews.json\n",
      "Movies fetched and saved to movies_reviews.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#test reviews scraper\n",
    "data = [\n",
    "    {\n",
    "        \"Movie ID\": \"tt17526714\",\n",
    "        \"Title\": \"The Substance\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open('test_movie.json', 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)\n",
    "\n",
    "path = os.path.join(os.getcwd(), 'test_movie.json')\n",
    "scraper = MovieReviewScraper(path)\n",
    "\n",
    "scraper.fetch_reviews()\n",
    "scraper.save_to_json()\n",
    "print(\"Movies fetched and saved to movies_reviews.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie ID: tt17526714 có 433 review(s).\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Đọc file JSON với mã hóa utf-8\n",
    "with open(os.path.join(os.getcwd(), 'movies_reviews.json'), 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Đếm số lượng review cho từng phim\n",
    "for movie in data:\n",
    "    movie_id = movie[\"Movie ID\"]\n",
    "    reviews = movie[\"Reviews\"]\n",
    "    review_count = len(reviews)\n",
    "    print(f\"Movie ID: {movie_id} có {review_count} review(s).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
