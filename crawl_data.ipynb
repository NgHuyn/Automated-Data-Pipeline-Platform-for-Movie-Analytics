{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping 20k Movies in IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.25.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.1.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from selenium) (2023.11.17)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from selenium) (4.9.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Requirement already satisfied: outcome in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\MODERN\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (1.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\MODERN\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\modern\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\MODERN\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "from bs4 import BeautifulSoup\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Base class for handling common WebDriver functionalities\n",
    "class BaseScraper:\n",
    "    def __init__(self):\n",
    "        self.driver = self.init_driver()\n",
    "\n",
    "    def init_driver(self):\n",
    "        service = Service()  # Initialize the service\n",
    "        options = webdriver.ChromeOptions()\n",
    "        \n",
    "        # Set Chrome options to reduce memory usage\n",
    "        options.headless = True  # Enable headless mode\n",
    "        options.add_argument('--disable-extensions')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument(\"--no-sandbox\")  # Use this if you encounter issues\n",
    "\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        return driver\n",
    "\n",
    "    def close_driver(self):\n",
    "        self.driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Movie Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class MoviesScraper(BaseScraper):\n",
    "    def __init__(self, clicks=200, batch_size=20, save_file='movies_data.json'):\n",
    "        super().__init__()\n",
    "        self.clicks = clicks\n",
    "        self.batch_size = batch_size\n",
    "        self.save_file = save_file\n",
    "        self.movie_data = []\n",
    "\n",
    "    def fetch_movies(self):\n",
    "        url = 'https://www.imdb.com/search/title/?title_type=feature&release_date=2024-01-01,2024-10-31'\n",
    "        self.driver.get(url)\n",
    "\n",
    "        total_batches = self.clicks // self.batch_size  # Calculate total batches\n",
    "        remaining_clicks = self.clicks % self.batch_size  # Remaining clicks after batches\n",
    "\n",
    "        for batch in range(total_batches):\n",
    "            try:\n",
    "                print(f\"Processing batch {batch + 1}/{total_batches}...\")\n",
    "                self._process_batch(self.batch_size)\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred in batch {batch + 1}: {e}\")\n",
    "                break # stop the process after an error\n",
    "\n",
    "        # Process any remaining clicks\n",
    "        if remaining_clicks > 0:\n",
    "            try:\n",
    "                print(f\"Processing remaining {remaining_clicks} clicks...\")\n",
    "                self._process_batch(remaining_clicks)\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred: {e}\")\n",
    "\n",
    "        self.close_driver()\n",
    "        return self.movie_data\n",
    "\n",
    "    def _process_batch(self, num_clicks):\n",
    "        \"\"\"Process a batch with num_clicks of 'see more' clicks.\"\"\"\n",
    "        initial_html = self.driver.page_source\n",
    "        initial_soup = BeautifulSoup(initial_html, 'html.parser')\n",
    "        initial_movies = initial_soup.select('div.sc-59c7dc1-2')  # Saving first element\n",
    "\n",
    "        # Extract data for the initial set of movies (phim ban đầu)\n",
    "        self.extract_movie_data(initial_movies)\n",
    "\n",
    "        with tqdm(total=num_clicks, desc='Loading movies') as pbar:\n",
    "            for _ in range(num_clicks):\n",
    "                soup = self.click_see_more_button()\n",
    "                pbar.update(1)\n",
    "\n",
    "        wait_time = self._calculate_wait_time(num_clicks)\n",
    "        time.sleep(wait_time)\n",
    "\n",
    "        # After pressing the 'see more' button\n",
    "        final_html = self.driver.page_source\n",
    "        final_soup = BeautifulSoup(final_html, 'html.parser')\n",
    "        final_movies = final_soup.select('div.sc-59c7dc1-2')\n",
    "\n",
    "        # After each batch, extract movie data (just extract new data)\n",
    "        new_movies = final_movies[len(initial_movies):]\n",
    "        self.extract_movie_data(new_movies)\n",
    "\n",
    "    def click_see_more_button(self):\n",
    "        try:\n",
    "            initial_elements = self.driver.find_elements(By.CLASS_NAME, 'ipc-title')\n",
    "            initial_count = len(initial_elements)\n",
    "\n",
    "            see_more_button = WebDriverWait(self.driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//span[contains(text(), '50 more')]\"))\n",
    "            )\n",
    "            self.driver.execute_script(\"arguments[0].scrollIntoView(); arguments[0].click();\", see_more_button)\n",
    "\n",
    "            for _ in range(5):\n",
    "                current_elements = self.driver.find_elements(By.CLASS_NAME, 'ipc-title')\n",
    "                current_count = len(current_elements)\n",
    "\n",
    "                if current_count > initial_count:\n",
    "                    break\n",
    "                time.sleep(1)\n",
    "\n",
    "            return BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "    def save_partial_data(self, batch_num):\n",
    "        \"\"\"Save temporary data after each batch.\"\"\"\n",
    "        temp_file_name = f\"movies_data_batch_{batch_num}.json\"\n",
    "        with open(temp_file_name, 'w', encoding='utf-8') as file:\n",
    "            json.dump(self.movie_data, file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Data for batch {batch_num} saved to {temp_file_name}\")\n",
    "\n",
    "    def extract_movie_data(self, movies):\n",
    "        existing_movie_ids = {movie['Movie ID'] for movie in self.movie_data}  # Set of existing movie IDs\n",
    "\n",
    "        for movie in movies:\n",
    "            title_tag = movie.select_one('h3.ipc-title__text')\n",
    "            link_tag = movie.select_one('a.ipc-title-link-wrapper')\n",
    "\n",
    "            title = title_tag.text.strip() if title_tag else 'N/A'\n",
    "            link = link_tag['href'] if link_tag else None\n",
    "\n",
    "            if link_tag:\n",
    "                movie_id = link.split('/title/')[1].split('/')[0]\n",
    "            else:\n",
    "                movie_id = 'N/A'\n",
    "\n",
    "            title = re.sub(r'^\\d+\\.\\s*', '', title)\n",
    "\n",
    "            if movie_id not in existing_movie_ids:  # Only add if movie_id is not already in the list\n",
    "                self.movie_data.append({\n",
    "                    'Movie ID': movie_id,\n",
    "                    'Title': title,\n",
    "                })\n",
    "\n",
    "    def _calculate_wait_time(self, clicks):\n",
    "        base_wait_time = 5\n",
    "        growth_factor = 1.2\n",
    "        additional_wait_time = base_wait_time * (growth_factor ** (clicks // 10))\n",
    "        return base_wait_time + additional_wait_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading movies: 100%|██████████| 4/4 [00:12<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 2/2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading movies: 100%|██████████| 4/4 [00:17<00:00,  4.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing remaining 2 clicks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading movies: 100%|██████████| 2/2 [00:11<00:00,  5.54s/it]\n"
     ]
    }
   ],
   "source": [
    "scraper = MoviesScraper(clicks=10, batch_size=4)\n",
    "movie_data = scraper.fetch_movies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def check_size(data):\n",
    "    # Kiểm tra kiểu dữ liệu và kích thước\n",
    "    if isinstance(data, list):\n",
    "        print(f\"Số lượng phần tử trong danh sách: {len(data)}\")\n",
    "\n",
    "        # Nếu phần tử là từ điển, kiểm tra thêm về trùng lặp và dữ liệu thiếu\n",
    "        if data:\n",
    "            first_element = data[0]\n",
    "            print(f\"Kích thước của phần tử đầu tiên: {len(first_element)} thuộc tính\")\n",
    "\n",
    "            # Kiểm tra trùng lặp (theo Movie ID)\n",
    "            movie_ids = [item.get('Movie ID') for item in data if 'Movie ID' in item]\n",
    "            duplicates = {movie_id for movie_id in movie_ids if movie_ids.count(movie_id) > 1}\n",
    "            if duplicates:\n",
    "                print(f\"Các Movie ID trùng lặp: {duplicates}\")\n",
    "            else:\n",
    "                print(\"Không có Movie ID trùng lặp.\")\n",
    "\n",
    "            # Kiểm tra dữ liệu thiếu\n",
    "            missing_values = []\n",
    "            for index, movie in enumerate(data):\n",
    "                for key, value in movie.items():\n",
    "                    if value in [None, '', 'N/A']:\n",
    "                        missing_values.append((index, key))\n",
    "            \n",
    "            if missing_values:\n",
    "                print(f\"Các giá trị thiếu (index, key): {missing_values}\")\n",
    "            else:\n",
    "                print(\"Không có giá trị thiếu.\")\n",
    "\n",
    "    elif isinstance(data, dict):\n",
    "        print(f\"Số lượng thuộc tính trong từ điển: {len(data)}\")\n",
    "    else:\n",
    "        print(\"Dữ liệu không phải là danh sách hoặc từ điển.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng phần tử trong danh sách: 550\n",
      "Kích thước của phần tử đầu tiên: 2 thuộc tính\n",
      "Không có Movie ID trùng lặp.\n",
      "Không có giá trị thiếu.\n"
     ]
    }
   ],
   "source": [
    "check_size(movie_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Reviews of each Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReviewsScraper class to fetch reviews for each movie\n",
    "class MovieReviewScraper(BaseScraper):\n",
    "    def __init__(self, movie_data):\n",
    "        super().__init__()  # Call the base class constructor\n",
    "        self.movie_data = movie_data  # Load movie data from movie data\n",
    "        self.movie_reviews = []  # Adjusted to be a list of movie objects\n",
    "        self.clicks = 0  # Initialize click counter\n",
    "\n",
    "    # def fetch_reviews(self):\n",
    "    #     for movie in self.movie_data:  # Iterate through the list of movies\n",
    "    #         movie_id = movie['Movie ID']\n",
    "    #         title = movie['Title']\n",
    "    #         review_url = f\"https://www.imdb.com/title/{movie_id}/reviews\"\n",
    "    #         self.driver.get(review_url)\n",
    "\n",
    "    #         self._load_reviews()\n",
    "\n",
    "    #         wait_time = self._calculate_wait_time(10, self.clicks)  # Adjust wait time based on click count\n",
    "    #         time.sleep(wait_time)\n",
    "\n",
    "    #         html = self.driver.page_source\n",
    "    #         soup = BeautifulSoup(html, 'html.parser')\n",
    "    #         self._extract_reviews(soup, movie_id, title)\n",
    "\n",
    "    #     self.close_driver()\n",
    "    #     return self.movie_reviews\n",
    "    \n",
    "    # def fetch_reviews(self):\n",
    "    #     for movie in self.movie_data:  # Iterate through the list of movies\n",
    "    #         movie_id = movie.get('Movie ID')  # Lấy Movie ID từ từ điển\n",
    "    #         title = movie.get('Title')  # Lấy Title từ từ điển\n",
    "\n",
    "    #         if movie_id and title:  # Kiểm tra nếu cả Movie ID và Title đều có giá trị\n",
    "    #             for i in range\n",
    "    #             review_url = f\"https://www.imdb.com/title/{movie_id}/reviews?sort=submissionDate&dir=desc&ratingFilter=0\"\n",
    "    #             print(f\"Fetching reviews for {title} ({movie_id}) at {review_url}\")\n",
    "    #             self.driver.get(review_url)\n",
    "\n",
    "    #             self._load_reviews()  # Load more reviews by clicking the button\n",
    "\n",
    "    #             wait_time = self._calculate_wait_time(10, self.clicks)  # Điều chỉnh thời gian chờ theo số lần click\n",
    "    #             time.sleep(wait_time)\n",
    "\n",
    "    #             html = self.driver.page_source\n",
    "    #             soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    #             # Trích xuất đánh giá cho phim hiện tại\n",
    "    #             self._extract_reviews(soup, movie_id, title)\n",
    "    #         else:\n",
    "    #             print(f\"Movie data missing for {movie}. Skipping this movie.\")\n",
    "\n",
    "    #     self.close_driver()\n",
    "    #     return self.movie_reviews\n",
    "\n",
    "    def fetch_reviews(self):\n",
    "        for movie in self.movie_data:  # Iterate through the list of movies\n",
    "            movie_id = movie.get('Movie ID')  # Lấy Movie ID từ từ điển\n",
    "            title = movie.get('Title')  # Lấy Title từ từ điển\n",
    "\n",
    "            if movie_id and title:  # Kiểm tra nếu cả Movie ID và Title đều có giá trị\n",
    "                for rating_filter in range(1, 11):  # Lặp từ 1 đến 10\n",
    "                    review_url = f\"https://www.imdb.com/title/{movie_id}/reviews?sort=submissionDate&dir=desc&ratingFilter={rating_filter}\"\n",
    "                    # print(f\"Fetching reviews for {title} ({movie_id}) at {review_url} with rating filter {rating_filter}\")\n",
    "                    self.driver.get(review_url)\n",
    "\n",
    "                    self._load_reviews()  # Load more reviews by clicking the button\n",
    "\n",
    "                    wait_time = self._calculate_wait_time(10, self.clicks)  # Điều chỉnh thời gian chờ theo số lần click\n",
    "                    time.sleep(wait_time)\n",
    "\n",
    "                    html = self.driver.page_source\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "                    # Trích xuất đánh giá cho phim hiện tại\n",
    "                    self._extract_reviews(soup, movie_id, title)\n",
    "            else:\n",
    "                print(f\"Movie data missing for {movie}. Skipping this movie.\")\n",
    "\n",
    "        self.close_driver()\n",
    "        return self.movie_reviews\n",
    "\n",
    "\n",
    "    def _load_reviews(self):\n",
    "        # Try to find and click the 'All' reviews button\n",
    "        try:\n",
    "            all_reviews_button = WebDriverWait(self.driver, 5).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//*[@id='__next']/main/div/section/div/section/div/div[1]/section[1]/div[3]/div/span[2]/button\"))\n",
    "            )\n",
    "            self.driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", all_reviews_button)\n",
    "            time.sleep(2)  # Wait for the page to load if needed\n",
    "            all_reviews_button.click()\n",
    "        except Exception as e:\n",
    "            print(\"Could not find 'All' button, will try to find 'Load More' button.\")\n",
    "            self._load_more_reviews()\n",
    "\n",
    "    # def _load_more_reviews(self):\n",
    "    #     # Add progress bar for loading more reviews\n",
    "    #     with tqdm(total=10, desc='Loading More Reviews', leave=False) as pbar:\n",
    "    #         while True:\n",
    "    #             try:\n",
    "    #                 load_more_button = WebDriverWait(self.driver, 5).until(\n",
    "    #                     EC.presence_of_element_located((By.XPATH, '//*[@id=\"load-more-trigger\"]'))\n",
    "    #                 )\n",
    "    #                 self.driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", load_more_button)\n",
    "    #                 load_more_button.click()\n",
    "\n",
    "    #                 self.clicks += 1  # Increment the click count\n",
    "    #                 pbar.update(1)  # Update progress bar\n",
    "\n",
    "    #                 wait_time = self._calculate_wait_time(1, self.clicks)  # Adjust wait time based on click count\n",
    "    #                 time.sleep(wait_time)\n",
    "\n",
    "    #             except Exception as e:\n",
    "    #                 print(\"No more 'Load More' buttons to click.\")\n",
    "    #                 break\n",
    "    \n",
    "    def _load_more_reviews(self):\n",
    "        # Add progress bar for loading more reviews\n",
    "        with tqdm(desc='Loading More Reviews', leave=False) as pbar:\n",
    "            while True:\n",
    "                try:\n",
    "                    load_more_button = WebDriverWait(self.driver, 5).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '//*[@id=\"load-more-trigger\"]'))\n",
    "                    )\n",
    "                    self.driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", load_more_button)\n",
    "                    load_more_button.click()\n",
    "\n",
    "                    self.clicks += 1  # Increment the click count\n",
    "                    pbar.update(1)  # Update progress bar\n",
    "\n",
    "                    wait_time = self._calculate_wait_time(1, self.clicks)  # Adjust wait time based on click count\n",
    "                    time.sleep(wait_time)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"No more 'Load More' buttons to click.\")\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "    def _calculate_wait_time(self, base_wait_time, clicks):\n",
    "        \"\"\"\n",
    "        Calculate an adaptive wait time based on the number of clicks.\n",
    "        As the number of clicks increases, the wait time grows exponentially to accommodate website lag.\n",
    "        \"\"\"\n",
    "        growth_factor = 1.2  # Exponential growth factor\n",
    "        additional_wait_time = base_wait_time * (growth_factor ** (clicks // 10))  # Increase wait time every 10 clicks\n",
    "        \n",
    "        return base_wait_time + additional_wait_time\n",
    "\n",
    "\n",
    "    def _extract_reviews(self, soup, movie_id, title):\n",
    "        reviews = soup.select('article.user-review-item')  # Attempt to extract reviews using one selector\n",
    "        movie_info = {\n",
    "            'Movie ID': movie_id,\n",
    "            'Reviews': []\n",
    "        }\n",
    "        # If no reviews found, try to load more reviews\n",
    "        if not reviews:  \n",
    "            reviews = soup.select('div.lister-item.mode-detail.imdb-user-review')\n",
    "            if not reviews: # If still no reviews available\n",
    "                print(f\"No reviews found for {title}.\")\n",
    "                return\n",
    "\n",
    "            for review in reviews:\n",
    "                parsed_review = self._parse_review(review, \"load_more\")\n",
    "                movie_info['Reviews'].append(parsed_review)\n",
    "        else: # If \"all\" button found\n",
    "            for review in reviews:\n",
    "                parsed_review = self._parse_review(review, \"all\")\n",
    "                movie_info['Reviews'].append(parsed_review)\n",
    "\n",
    "        self.movie_reviews.append(movie_info)\n",
    "\n",
    "        # Count the number of reviews and display it\n",
    "        num_reviews = len(movie_info['Reviews'])\n",
    "        print(f\"Total number of reviews for '{title}': {num_reviews}\")\n",
    "\n",
    "    def _parse_review(self, review, button_type):\n",
    "        \"\"\"\n",
    "        Extract information from the review and return as a dictionary.\n",
    "        \"\"\"\n",
    "        # Extract information from the review based on its type (load_more or all)\n",
    "        if button_type == \"load_more\":\n",
    "            review_rating = review.select_one('span.rating-other-user-rating span').get_text(strip=True) if review.select_one('span.rating-other-user-rating span') else 'No rating'\n",
    "            review_summary = review.select_one('a.title').get_text(strip=True) if review.select_one('a.title') else 'No summary'\n",
    "            review_text = review.select_one('div.text.show-more__control').get_text(strip=True) if review.select_one('div.text.show-more__control') else 'No content'\n",
    "            author_tag = review.select_one('span.display-name-link a').get_text(strip=True) if review.select_one('span.display-name-link a') else 'Unknown Author'\n",
    "            review_date = review.select_one('span.review-date').get_text(strip=True) if review.select_one('span.review-date') else 'No date'\n",
    "        else:\n",
    "            review_rating = review.select_one('span.ipc-rating-star--rating').get_text(strip=True) if review.select_one('span.ipc-rating-star--rating') else 'No rating'\n",
    "            review_summary = review.select_one('span[data-testid=\"review-summary\"]').get_text(strip=True) if review.select_one('span[data-testid=\"review-summary\"]') else 'No summary'\n",
    "            review_text = review.select_one('div.ipc-html-content-inner-div').get_text(strip=True) if review.select_one('div.ipc-html-content-inner-div') else 'No content'\n",
    "            author_tag = review.select_one('a[data-testid=\"author-link\"]').get_text(strip=True) if review.select_one('a[data-testid=\"author-link\"]') else 'Unknown Author'\n",
    "            review_date = review.select_one('li.review-date').get_text(strip=True) if review.select_one('li.review-date') else 'No date'\n",
    "\n",
    "        # Return the review information in the expected format\n",
    "        return {\n",
    "            'Review Summary': review_summary,\n",
    "            'Review': review_text,\n",
    "            'Rating': review_rating,\n",
    "            'Author': author_tag,\n",
    "            'Date': review_date\n",
    "        }\n",
    "    def save_to_json(self):\n",
    "        with open('movies_reviews.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.movie_reviews, f, ensure_ascii=False, indent=4)\n",
    "        print(\"Reviews saved to movies_reviews.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1 movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Movie ID': 'tt11315808', 'Title': 'Joker: Folie à Deux'},\n",
       " {'Movie ID': 'tt17526714', 'Title': 'The Substance'},\n",
       " {'Movie ID': 'tt10128846', 'Title': 'Megalopolis'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find 'All' button, will try to find 'Load More' button.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading More Reviews: 1it [00:00,  4.17it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "scraper = MovieReviewScraper(movie_data=movie_data[0:3])\n",
    "\n",
    "scraper.fetch_reviews()\n",
    "scraper.save_to_json()\n",
    "print(\"Movies fetched and saved to movies_reviews.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie ID: tt11315808 có 425 review(s).\n",
      "Movie ID: tt17526714 có 425 review(s).\n",
      "Movie ID: tt10128846 có 294 review(s).\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Đọc file JSON với mã hóa utf-8\n",
    "with open(os.path.join(os.getcwd(), 'movies_reviews.json'), 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Đếm số lượng review cho từng phim\n",
    "for movie in data:\n",
    "    movie_id = movie[\"Movie ID\"]\n",
    "    reviews = movie[\"Reviews\"]\n",
    "    review_count = len(reviews)\n",
    "    print(f\"Movie ID: {movie_id} có {review_count} review(s).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
